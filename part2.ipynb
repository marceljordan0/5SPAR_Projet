{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d806ad3-fba2-45ba-a42a-6f4dcb52514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark psycopg2-binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de3d3e3-8399-4f33-a314-4107525ce47a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vérification et création de la base de données si nécessaire...\n",
      "La base de données 'mastodon_data' existe déjà.\n",
      "Création des tables dans PostgreSQL...\n",
      "Tables créées avec succès dans PostgreSQL.\n",
      "Initialisation de SparkSession...\n",
      "SparkSession initialisée.\n",
      "Lecture des flux de Kafka...\n",
      "Flux de Kafka configuré.\n",
      "Parsing des données JSON...\n",
      "Parsing des données JSON terminé.\n",
      "Nettoyage des données...\n",
      "Nettoyage des données terminé.\n",
      "Agrégation des données en fenêtres temporelles...\n",
      "Agrégation des données terminée.\n",
      "Début de l'écriture des toots filtrés...\n",
      "Écriture des toots filtrés démarrée.\n",
      "Début de l'écriture des agrégats de fenêtres temporelles...\n",
      "Écriture des agrégats de fenêtres temporelles démarrée.\n",
      "Début de l'écriture de la longueur moyenne des toots par utilisateur...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Écriture de la longueur moyenne des toots par utilisateur démarrée.\n",
      "Début de l'écriture de la longueur moyenne des toots par hashtag...\n",
      "Écriture de la longueur moyenne des toots par hashtag démarrée.\n",
      "Tentative d'écriture du batch 0 dans la table 'filtered_toots'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch 0 écrit avec succès dans 'filtered_toots'.\n",
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p5\n",
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 écrit avec succès dans 'filtered_toots'.\n",
      "Tentative d'écriture du batch 0 dans la table 'toots_time_window_aggregates'...\n",
      "Tentative d'écriture du batch 0 dans la table 'user_avg_length'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:py4j.clientserver:Python Server ready to receive messages\n",
      "INFO:py4j.clientserver:Received command c on object id p7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tentative d'écriture du batch 0 dans la table 'hashtag_avg_length'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch 0 écrit avec succès dans 'toots_time_window_aggregates'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 écrit avec succès dans 'toots_time_window_aggregates'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch 0 écrit avec succès dans 'user_avg_length'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 écrit avec succès dans 'user_avg_length'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Batch 0 écrit avec succès dans 'hashtag_avg_length'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0 écrit avec succès dans 'hashtag_avg_length'.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, TimestampType, IntegerType, ArrayType, FloatType\n",
    ")\n",
    "from pyspark.sql.functions import (\n",
    "    from_json, col, lower, regexp_replace, window, avg, count, length, explode, concat_ws\n",
    ")\n",
    "import psycopg2\n",
    "from psycopg2 import sql\n",
    "\n",
    "# Configure le logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Paramètres de connexion à PostgreSQL\n",
    "db_host = \"some-postgres\"\n",
    "db_port = \"5432\"\n",
    "db_name = \"mastodon_data\"\n",
    "db_user = \"postgres\"\n",
    "db_password = \"mysecretpassword\"\n",
    "db_url = f\"jdbc:postgresql://{db_host}:{db_port}/{db_name}\"\n",
    "db_properties = {\n",
    "    \"user\": db_user,\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "\n",
    "# Fonction pour créer la base de données si elle n'existe pas\n",
    "def create_database():\n",
    "    print(\"Vérification et création de la base de données si nécessaire...\")\n",
    "    try:\n",
    "        # Se connecte à la base de données \n",
    "        conn = psycopg2.connect(\n",
    "            dbname='postgres',\n",
    "            user=db_properties[\"user\"],\n",
    "            password=db_properties[\"password\"],\n",
    "            host=db_host,\n",
    "            port=db_port\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        # Vérifie si la base de données existe\n",
    "        cursor.execute(f\"SELECT 1 FROM pg_catalog.pg_database WHERE datname = '{db_name}'\")\n",
    "        exists = cursor.fetchone()\n",
    "        if not exists:\n",
    "            # Crée la base de données\n",
    "            cursor.execute(f'CREATE DATABASE {db_name}')\n",
    "            print(f\"Base de données '{db_name}' créée avec succès.\")\n",
    "        else:\n",
    "            print(f\"La base de données '{db_name}' existe déjà.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lors de la vérification/création de la base de données: {e}\")\n",
    "        print(f\"Erreur lors de la vérification/création de la base de données: {e}\")\n",
    "\n",
    "\n",
    "# Fonction pour créer les tables dans PostgreSQL\n",
    "def create_tables():\n",
    "    print(\"Création des tables dans PostgreSQL...\")\n",
    "    try:\n",
    "        conn = psycopg2.connect(\n",
    "            dbname=db_name,\n",
    "            user=db_properties[\"user\"],\n",
    "            password=db_properties[\"password\"],\n",
    "            host=db_host,\n",
    "            port=db_port\n",
    "        )\n",
    "        conn.autocommit = True\n",
    "        cursor = conn.cursor()\n",
    "\n",
    "        create_filtered_toots_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS filtered_toots (\n",
    "            toot_id VARCHAR(255) PRIMARY KEY,\n",
    "            timestamp TIMESTAMP,\n",
    "            text TEXT,\n",
    "            user_id VARCHAR(255),\n",
    "            language VARCHAR(10),\n",
    "            hashtags TEXT,\n",
    "            reblogs_count INTEGER,\n",
    "            favourites_count INTEGER,\n",
    "            replies_count INTEGER\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        create_time_window_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS toots_time_window_aggregates (\n",
    "            window_start TIMESTAMP,\n",
    "            window_end TIMESTAMP,\n",
    "            total_toots INTEGER,\n",
    "            PRIMARY KEY (window_start, window_end)\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        create_user_avg_length_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS user_avg_length (\n",
    "            user_id VARCHAR(255) PRIMARY KEY,\n",
    "            average_length FLOAT\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        create_hashtag_avg_length_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS hashtag_avg_length (\n",
    "            hashtag VARCHAR(255) PRIMARY KEY,\n",
    "            average_length FLOAT\n",
    "        );\n",
    "        \"\"\"\n",
    "\n",
    "        cursor.execute(create_filtered_toots_table)\n",
    "        cursor.execute(create_time_window_table)\n",
    "        cursor.execute(create_user_avg_length_table)\n",
    "        cursor.execute(create_hashtag_avg_length_table)\n",
    "\n",
    "        print(\"Tables créées avec succès dans PostgreSQL.\")\n",
    "\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lors de la création des tables dans PostgreSQL: {e}\")\n",
    "        print(f\"Erreur lors de la création des tables: {e}\")\n",
    "\n",
    "# Crée la base de données si elle n'existe pas\n",
    "create_database()\n",
    "# Crée les tables\n",
    "create_tables()\n",
    "\n",
    "# Configure SparkSession avec le package Kafka et le driver JDBC PostgreSQL\n",
    "print(\"Initialisation de SparkSession...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MastodonStreaming\") \\\n",
    "    .config(\n",
    "        \"spark.jars.packages\",\n",
    "        \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0,\"\n",
    "        \"org.postgresql:postgresql:42.2.25\"\n",
    "    ) \\\n",
    "    .getOrCreate()\n",
    "print(\"SparkSession initialisée.\")\n",
    "\n",
    "# Défini les paramètres de Kafka\n",
    "kafka_brokers = \"kafka:9092\"\n",
    "topic_name = \"mastodon_stream\"\n",
    "\n",
    "print(\"Lecture des flux de Kafka...\")\n",
    "# Li les flux de Kafka\n",
    "raw_stream = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_brokers) \\\n",
    "    .option(\"subscribe\", topic_name) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "print(\"Flux de Kafka configuré.\")\n",
    "\n",
    "# Schéma pour les données JSON\n",
    "schema = StructType([\n",
    "    StructField(\"id\", StringType()),\n",
    "    StructField(\"created_at\", TimestampType()),\n",
    "    StructField(\"content\", StringType()),\n",
    "    StructField(\"user_id\", StringType()),\n",
    "    StructField(\"language\", StringType()),  \n",
    "    StructField(\"tags\", ArrayType(StringType())),\n",
    "    StructField(\"reblogs_count\", IntegerType()),\n",
    "    StructField(\"favourites_count\", IntegerType()),\n",
    "    StructField(\"replies_count\", IntegerType())\n",
    "])\n",
    "\n",
    "print(\"Parsing des données JSON...\")\n",
    "# Parser les données JSON\n",
    "json_stream = raw_stream.selectExpr(\"CAST(value AS STRING) as json_value\")\n",
    "parsed_stream = json_stream.withColumn(\"data\", from_json(col(\"json_value\"), schema)).select(\"data.*\")\n",
    "print(\"Parsing des données JSON terminé.\")\n",
    "\n",
    "# Nettoi les données\n",
    "print(\"Nettoyage des données...\")\n",
    "cleaned_stream = parsed_stream \\\n",
    "    .filter(col(\"content\").isNotNull()) \\\n",
    "    .withColumn(\"text\", regexp_replace(col(\"content\"), r\"<[^>]+>\", \"\")) \\\n",
    "    .withColumn(\"text\", lower(col(\"text\"))) \\\n",
    "    .withColumn(\"text\", regexp_replace(col(\"text\"), r\"http\\S+\", \"\")) \\\n",
    "    .withColumn(\"text\", regexp_replace(col(\"text\"), r\"\\s+\", \" \")) \\\n",
    "    .withColumn(\"text\", regexp_replace(col(\"text\"), r\"[^\\w\\s#]\", \"\")) \\\n",
    "    .withColumn(\"text_length\", length(col(\"text\")))\n",
    "print(\"Nettoyage des données terminé.\")\n",
    "\n",
    "# Extrai user_id depuis account.id\n",
    "cleaned_stream = cleaned_stream.withColumn(\"user_id\", col(\"user_id\"))\n",
    "\n",
    "# Extrai les hashtags sous forme de tableau de chaînes\n",
    "cleaned_stream = cleaned_stream.withColumn(\"hashtags\", concat_ws(\",\", col(\"tags\")))\n",
    "\n",
    "# filtre par langue\n",
    "cleaned_stream = cleaned_stream.filter(col(\"language\") == \"fr\")\n",
    "\n",
    "\n",
    "# Prépare les données pour la table des toots filtrés\n",
    "filtered_toots = cleaned_stream.select(\n",
    "    col('id').alias('toot_id'),\n",
    "    col('created_at').alias('timestamp'),\n",
    "    col('text'),\n",
    "    col('user_id'),\n",
    "    col('language'),\n",
    "    col('hashtags'),\n",
    "    col('reblogs_count'),\n",
    "    col('favourites_count'),\n",
    "    col('replies_count')\n",
    ")\n",
    "\n",
    "# Converti les hashtags en chaîne de caractères pour stockage\n",
    "filtered_toots = filtered_toots.withColumn(\"hashtags\", concat_ws(\",\", col(\"hashtags\")))\n",
    "\n",
    "# Agrégation des données en fenêtres temporelles\n",
    "print(\"Agrégation des données en fenêtres temporelles...\")\n",
    "windowed_stream = cleaned_stream \\\n",
    "    .withWatermark(\"created_at\", \"1 hour\") \\\n",
    "    .groupBy(window(col(\"created_at\"), \"1 hour\")) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_toots\")\n",
    "    )\n",
    "print(\"Agrégation des données terminée.\")\n",
    "\n",
    "# Prépare les données pour la table des agrégats de fenêtres temporelles\n",
    "windowed_aggregates = windowed_stream.select(\n",
    "    col('window.start').alias('window_start'),\n",
    "    col('window.end').alias('window_end'),\n",
    "    col('total_toots')\n",
    ")\n",
    "\n",
    "# Calcul de la longueur moyenne des toots par utilisateur\n",
    "user_avg_length = cleaned_stream \\\n",
    "    .filter(col(\"user_id\").isNotNull()) \\\n",
    "    .groupBy('user_id') \\\n",
    "    .agg(\n",
    "        avg('text_length').alias('average_length')\n",
    "    )\n",
    "\n",
    "\n",
    "# Calcul de la longueur moyenne des toots par hashtag\n",
    "hashtag_avg_length = cleaned_stream \\\n",
    "    .withColumn('hashtag', explode(col('tags'))) \\\n",
    "    .groupBy('hashtag') \\\n",
    "    .agg(\n",
    "        avg('text_length').alias('average_length')\n",
    "    )\n",
    "\n",
    "\n",
    "# Fonction pour écrire dans PostgreSQL\n",
    "def write_to_postgres(df, epoch_id, table_name, save_mode=\"append\"):\n",
    "    print(f\"Tentative d'écriture du batch {epoch_id} dans la table '{table_name}'...\")\n",
    "    try:\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", db_url) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", db_user) \\\n",
    "            .option(\"password\", db_password) \\\n",
    "            .option(\"driver\", db_properties[\"driver\"]) \\\n",
    "            .mode(save_mode) \\\n",
    "            .save()\n",
    "        logging.info(f\"Batch {epoch_id} écrit avec succès dans '{table_name}'.\")\n",
    "        print(f\"Batch {epoch_id} écrit avec succès dans '{table_name}'.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lors de l'écriture dans '{table_name}' : {e}\")\n",
    "        print(f\"Erreur lors de l'écriture dans '{table_name}' : {e}\")\n",
    "\n",
    "# Démarre les écritures en streaming\n",
    "print(\"Début de l'écriture des toots filtrés...\")\n",
    "query_filtered_toots = filtered_toots.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(lambda df, epoch_id: write_to_postgres(df, epoch_id, \"filtered_toots\")) \\\n",
    "    .start()\n",
    "print(\"Écriture des toots filtrés démarrée.\")\n",
    "\n",
    "print(\"Début de l'écriture des agrégats de fenêtres temporelles...\")\n",
    "query_time_window = windowed_aggregates.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .foreachBatch(lambda df, epoch_id: write_to_postgres(df, epoch_id, \"toots_time_window_aggregates\")) \\\n",
    "    .start()\n",
    "print(\"Écriture des agrégats de fenêtres temporelles démarrée.\")\n",
    "\n",
    "print(\"Début de l'écriture de la longueur moyenne des toots par utilisateur...\")\n",
    "query_user_avg_length = user_avg_length.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(lambda df, epoch_id: write_to_postgres(df, epoch_id, \"user_avg_length\", save_mode=\"overwrite\")) \\\n",
    "    .start()\n",
    "print(\"Écriture de la longueur moyenne des toots par utilisateur démarrée.\")\n",
    "\n",
    "print(\"Début de l'écriture de la longueur moyenne des toots par hashtag...\")\n",
    "query_hashtag_avg_length = hashtag_avg_length.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .foreachBatch(lambda df, epoch_id: write_to_postgres(df, epoch_id, \"hashtag_avg_length\", save_mode=\"overwrite\")) \\\n",
    "    .start()\n",
    "print(\"Écriture de la longueur moyenne des toots par hashtag démarrée.\")\n",
    "\n",
    "# Attend la fin des requêtes\n",
    "query_filtered_toots.awaitTermination()\n",
    "query_time_window.awaitTermination()\n",
    "query_user_avg_length.awaitTermination()\n",
    "query_hashtag_avg_length.awaitTermination()\n",
    "\n",
    "print(\"Streaming terminé.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c7b7bb3-31dc-42ad-b697-d631c1f12d8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
