{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8466eac6-ad7b-4506-b095-831b07f231ab",
   "metadata": {},
   "source": [
    "!pip install --upgrade py4j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aacf6da1-e58c-412a-85c9-7f9337b3980f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables créées ou déjà existantes.\n",
      "Charge les données historiques depuis PostgreSQL (history_toots)...\n",
      "Nombre de toots déjà traités (historique) : 0\n",
      "Charge les nouveaux toots depuis PostgreSQL (filtered_toots)...\n",
      "Nombre de nouveaux toots : 2\n",
      "Chargement des données étiquetées pour l'entraînement...\n",
      "Nombre de lignes après suppression des valeurs nulles : 1600000\n",
      "Tokenisation du texte...\n",
      "Vectorisation du texte...\n",
      "Division des données en ensembles d'entraînement et de test...\n",
      "Nombre de lignes d'entraînement : 128069\n",
      "Nombre de lignes de test : 31671\n",
      "Entraînement du modèle de régression logistique...\n",
      "Modèle de régression logistique entraîné.\n",
      "Prédiction sur les données de test...\n",
      "Aperçu des prédictions :\n",
      "+--------------------+-----+----------+\n",
      "|                text|label|prediction|\n",
      "+--------------------+-----+----------+\n",
      "|@LettyA ahh ive a...|    0|       0.0|\n",
      "|Damm back to scho...|    0|       0.0|\n",
      "|wonders why someo...|    0|       0.0|\n",
      "|Emily will be gla...|    0|       0.0|\n",
      "|@machineplay I'm ...|    0|       0.0|\n",
      "+--------------------+-----+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Évaluation de la précision du modèle...\n",
      "Précision du modèle : 0.713333964825866\n",
      "Tokenise les nouveaux toots...\n",
      "Vectorise les nouveaux toots...\n",
      "Prédit les sentiments sur les nouveaux toots...\n",
      "Mappe les prédictions à des labels de sentiment...\n",
      "Aperçu des résultats avec les labels de sentiment :\n",
      "+------------------+--------------------+----------+---------+\n",
      "|           toot_id|                text|prediction|sentiment|\n",
      "+------------------+--------------------+----------+---------+\n",
      "|113284085642520328|i tried getting c...|       0.0|  négatif|\n",
      "|113284089790341845|boss i know engli...|       4.0|  positif|\n",
      "+------------------+--------------------+----------+---------+\n",
      "\n",
      "Stocke les résultats dans PostgreSQL (toots_sentiments)...\n",
      "Résultats stockés avec succès.\n",
      "Déplace les toots traités dans PostgreSQL (history_toots)...\n",
      "Toots déplacés dans la table historique avec succès.\n",
      "Supprime les toots traités de la table filtered_toots...\n",
      "Aucun toot_id à supprimer.\n",
      "Arrêt de SparkSession...\n",
      "SparkSession arrêtée.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.ml.feature import Tokenizer, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "import psycopg2\n",
    "\n",
    "# Paramètres de connexion à PostgreSQL\n",
    "db_host = \"some-postgres\"\n",
    "db_port = \"5432\"\n",
    "db_name = \"mastodon_data\"\n",
    "db_user = \"postgres\"\n",
    "db_password = \"mysecretpassword\"\n",
    "db_url = f\"jdbc:postgresql://{db_host}:{db_port}/{db_name}\"\n",
    "db_properties = {\n",
    "    \"user\": db_user,\n",
    "    \"password\": db_password,\n",
    "    \"driver\": \"org.postgresql.Driver\"\n",
    "}\n",
    "\n",
    "# Initialise SparkSession avec des ajustements de mémoire\n",
    "spark = SparkSession.builder.appName(\"TootsSentimentAnalysis\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.postgresql:postgresql:42.2.25\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Crée les tables si elles n'existent pas\n",
    "def create_tables():\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host, port=db_port, dbname=db_name, user=db_user, password=db_password)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    create_history_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS history_toots (\n",
    "        toot_id VARCHAR(255) PRIMARY KEY,\n",
    "        timestamp TIMESTAMP,\n",
    "        text TEXT,\n",
    "        user_id VARCHAR(255),\n",
    "        language VARCHAR(10),\n",
    "        hashtags TEXT,\n",
    "        reblogs_count INTEGER,\n",
    "        favourites_count INTEGER,\n",
    "        replies_count INTEGER\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    create_sentiments_table = \"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS toots_sentiments (\n",
    "        toot_id VARCHAR(255) PRIMARY KEY,\n",
    "        text TEXT,\n",
    "        prediction DOUBLE PRECISION,\n",
    "        sentiment VARCHAR(20)\n",
    "    );\n",
    "    \"\"\"\n",
    "\n",
    "    cursor.execute(create_history_table)\n",
    "    cursor.execute(create_sentiments_table)\n",
    "    conn.commit()\n",
    "    cursor.close()\n",
    "    conn.close()\n",
    "\n",
    "    print(\"Tables créées ou déjà existantes.\")\n",
    "\n",
    "# Charge les données historiques depuis la table history_toots\n",
    "def load_historical_data():\n",
    "    print(\"Charge les données historiques depuis PostgreSQL (history_toots)...\")\n",
    "    try:\n",
    "        historical_data = spark.read.jdbc(url=db_url, table=\"history_toots\", properties=db_properties)\n",
    "        print(f\"Nombre de toots déjà traités (historique) : {historical_data.count()}\")\n",
    "        return historical_data\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données historiques : {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction pour charger les données d'entraînement\n",
    "def load_training_data():\n",
    "    try:\n",
    "        print(\"Chargement des données étiquetées pour l'entraînement...\")\n",
    "        data = spark.read.csv(\"tweets_dataset.csv\", header=False, inferSchema=True)\n",
    "        data = data.toDF(\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\")\n",
    "        data = data.withColumn(\"target\", data[\"target\"].cast(IntegerType()))\n",
    "        data = data.withColumn(\"label\", data[\"target\"])\n",
    "        data = data.na.drop(subset=[\"text\", \"label\"])\n",
    "        print(f\"Nombre de lignes après suppression des valeurs nulles : {data.count()}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des données d'entraînement : {e}\")\n",
    "        return None\n",
    "\n",
    "# Fonction pour tokeniser et vectoriser les données\n",
    "def preprocess_data(data):\n",
    "    try:\n",
    "        print(\"Tokenisation du texte...\")\n",
    "        tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "        tokenized_data = tokenizer.transform(data)\n",
    "\n",
    "        print(\"Vectorisation du texte...\")\n",
    "        vectorizer = CountVectorizer(inputCol=\"words\", outputCol=\"features\")\n",
    "        vectorizer_model = vectorizer.fit(tokenized_data)\n",
    "        vectorized_data = vectorizer_model.transform(tokenized_data)\n",
    "\n",
    "        return vectorized_data, vectorizer_model\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du prétraitement des données : {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Fonction pour entraîner le modèle\n",
    "def train_logistic_regression(vectorized_data):\n",
    "    try:\n",
    "        print(\"Division des données en ensembles d'entraînement et de test...\")\n",
    "        (training_data, test_data) = vectorized_data.sample(fraction=0.1, seed=42).randomSplit([0.8, 0.2], seed=42)\n",
    "        print(f\"Nombre de lignes d'entraînement : {training_data.count()}\")\n",
    "        print(f\"Nombre de lignes de test : {test_data.count()}\")\n",
    "\n",
    "        print(\"Entraînement du modèle de régression logistique...\")\n",
    "        lr = LogisticRegression(featuresCol='features', labelCol='label', maxIter=10)\n",
    "        lr_model = lr.fit(training_data)\n",
    "        print(\"Modèle de régression logistique entraîné.\")\n",
    "        return lr_model, test_data\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'entraînement du modèle : {e}\")\n",
    "        return None, None\n",
    "\n",
    "# Fonction pour évaluer le modèle\n",
    "def evaluate_model(lr_model, test_data):\n",
    "    try:\n",
    "        print(\"Prédiction sur les données de test...\")\n",
    "        predictions = lr_model.transform(test_data)\n",
    "        print(\"Aperçu des prédictions :\")\n",
    "        predictions.select(\"text\", \"label\", \"prediction\").show(5)\n",
    "\n",
    "        print(\"Évaluation de la précision du modèle...\")\n",
    "        evaluator = MulticlassClassificationEvaluator(\n",
    "            labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "        accuracy = evaluator.evaluate(predictions)\n",
    "        print(f\"Précision du modèle : {accuracy}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'évaluation du modèle : {e}\")\n",
    "\n",
    "\n",
    "# Charge les nouveaux toots qui ne sont pas dans history_toots\n",
    "def load_new_toots(historical_data):\n",
    "    print(\"Charge les nouveaux toots depuis PostgreSQL (filtered_toots)...\")\n",
    "    try:\n",
    "        all_toots = spark.read.jdbc(url=db_url, table=\"filtered_toots\", properties=db_properties)\n",
    "        new_toots = all_toots.join(historical_data, on=\"toot_id\", how=\"left_anti\")  \n",
    "        print(f\"Nombre de nouveaux toots : {new_toots.count()}\")\n",
    "        return new_toots\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement des nouveaux toots : {e}\")\n",
    "        return None\n",
    "\n",
    "# Prédit les sentiments sur les nouveaux toots\n",
    "def predict_sentiments(lr_model, vectorizer_model, new_toots):\n",
    "    try:\n",
    "        print(\"Tokenise les nouveaux toots...\")\n",
    "        tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "        tokenized_toots = tokenizer.transform(new_toots)\n",
    "\n",
    "        print(\"Vectorise les nouveaux toots...\")\n",
    "        vectorized_toots = vectorizer_model.transform(tokenized_toots)\n",
    "\n",
    "        print(\"Prédit les sentiments sur les nouveaux toots...\")\n",
    "        sentiment_predictions = lr_model.transform(vectorized_toots)\n",
    "        results = sentiment_predictions.select(\"toot_id\", \"text\", \"prediction\")\n",
    "\n",
    "        # Mappe les prédictions à des labels de sentiment\n",
    "        print(\"Mappe les prédictions à des labels de sentiment...\")\n",
    "        def sentiment_label(prediction):\n",
    "            if prediction == 0.0:\n",
    "                return \"négatif\"\n",
    "            elif prediction == 2.0:\n",
    "                return \"neutre\"\n",
    "            elif prediction == 4.0:\n",
    "                return \"positif\"\n",
    "            else:\n",
    "                return \"inconnu\"\n",
    "\n",
    "        sentiment_udf = udf(sentiment_label, StringType())\n",
    "        results = results.withColumn(\"sentiment\", sentiment_udf(results.prediction))\n",
    "        print(\"Aperçu des résultats avec les labels de sentiment :\")\n",
    "        results.show(5)\n",
    "\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la prédiction des sentiments : {e}\")\n",
    "        return None\n",
    "\n",
    "# Stocke les résultats dans toots_sentiments et déplace les toots dans history_toots\n",
    "def store_results_in_postgres(results):\n",
    "    print(\"Stocke les résultats dans PostgreSQL (toots_sentiments)...\")\n",
    "    try:\n",
    "        results.write.jdbc(url=db_url, table=\"toots_sentiments\", mode=\"append\", properties=db_properties)\n",
    "        print(\"Résultats stockés avec succès.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du stockage des résultats : {e}\")\n",
    "\n",
    "# Déplace les toots traités dans history_toots\n",
    "def move_toots_to_history(new_toots):\n",
    "    print(\"Déplace les toots traités dans PostgreSQL (history_toots)...\")\n",
    "    try:\n",
    "        new_toots.select(\"toot_id\", \"timestamp\", \"text\", \"user_id\", \"language\", \"hashtags\", \"reblogs_count\", \"favourites_count\", \"replies_count\").write.jdbc(url=db_url, table=\"history_toots\", mode=\"append\", properties=db_properties)\n",
    "        print(\"Toots déplacés dans la table historique avec succès.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du déplacement des toots vers l'historique : {e}\")\n",
    "\n",
    "# Supprime les toots traités de filtered_toots\n",
    "def delete_processed_toots(new_toots):\n",
    "    print(\"Supprime les toots traités de la table filtered_toots...\")\n",
    "    conn = psycopg2.connect(\n",
    "        host=db_host, port=db_port, dbname=db_name, user=db_user, password=db_password)\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        toot_ids = [row.toot_id for row in new_toots.select(\"toot_id\").collect()]\n",
    "        if toot_ids:\n",
    "            formatted_ids = ','.join([f\"'{toot_id}'\" for toot_id in toot_ids])\n",
    "            query = f\"DELETE FROM filtered_toots WHERE toot_id IN ({formatted_ids})\"\n",
    "            print(f\"Exécution de la requête : {query}\")  \n",
    "            cursor.execute(query)\n",
    "            conn.commit()\n",
    "            print(f\"Toots supprimés de filtered_toots : {len(toot_ids)}\")\n",
    "        else:\n",
    "            print(\"Aucun toot_id à supprimer.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la suppression des toots traités : {e}\")\n",
    "    finally:\n",
    "        cursor.close()\n",
    "        conn.close()\n",
    "\n",
    "\n",
    "# Pipeline principal\n",
    "def main():\n",
    "    # Crée les tables si elles n'existent pas\n",
    "    create_tables()\n",
    "\n",
    "    # Charge les toots déjà traités (historique)\n",
    "    historical_data = load_historical_data()\n",
    "    if historical_data is None:\n",
    "        return\n",
    "\n",
    "    # Charge les nouveaux toots qui ne sont pas dans history_toots\n",
    "    new_toots = load_new_toots(historical_data)\n",
    "    if new_toots is None or new_toots.count() == 0:\n",
    "        print(\"Aucun nouveau toot à traiter.\")\n",
    "        return\n",
    "\n",
    "    # Charge les données d'entraînement et entraîne le modèle\n",
    "    data = load_training_data() \n",
    "    if data is None:\n",
    "        return\n",
    "\n",
    "    vectorized_data, vectorizer_model = preprocess_data(data)\n",
    "    if vectorized_data is None:\n",
    "        return\n",
    "\n",
    "    lr_model, test_data = train_logistic_regression(vectorized_data)\n",
    "    if lr_model is not None:\n",
    "        evaluate_model(lr_model, test_data)\n",
    "\n",
    "        # Prédit les sentiments sur les nouveaux toots\n",
    "        results = predict_sentiments(lr_model, vectorizer_model, new_toots)\n",
    "        if results is not None:\n",
    "            store_results_in_postgres(results)\n",
    "            move_toots_to_history(new_toots)  \n",
    "            delete_processed_toots(new_toots)  \n",
    "\n",
    "    # Arrête SparkSession\n",
    "    print(\"Arrêt de SparkSession...\")\n",
    "    spark.stop()\n",
    "    print(\"SparkSession arrêtée.\")\n",
    "\n",
    "# Exécute le pipeline principal\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7fe109-d1ed-4e73-b982-c8571597f9a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
