# ğŸ—ƒï¸ 5SPAR ğŸ“Š Streaming ğŸ› ï¸

## ğŸ—ï¸ **Architecture et Setup** âš™ï¸

Tous les composants du projet sont conteneurisÃ©s ğŸ“¦ et orchestrÃ©s avec Docker ğŸ‹ et Docker Compose ğŸ“œ pour faciliter le dÃ©ploiement et la configuration. Voici les Ã©tapes pour initialiser les environnements ğŸŒ et le rÃ©seau :

### ğŸš€ **Lancer les containers Jupyter ğŸ““, Spark (master et worker) âœ¨, Kafka ğŸ“¨, et Zookeeper ğŸ¾ avec Docker Compose :**

```sh
   docker-compose up
```

### ğŸ˜ **CrÃ©er un container PostgreSQL :**

```sh
   docker run --name some-postgres -e POSTGRES_PASSWORD=mysecretpassword -v some-postgres-data:/var/lib/postgresql/data -d -p 5432:5432 postgres
```

### ğŸŒ **CrÃ©er un rÃ©seau Docker et y connecter les containers :**

```sh
   docker network create bigdata_network
   docker network connect bigdata_network some-postgres
   docker network connect bigdata_network jupyter
   docker network connect bigdata_network spark-worker
   docker network connect bigdata_network kafka
   docker network connect bigdata_network spark-master
   docker network connect bigdata_network zookeeper
```

## ğŸ”‘ **AccÃ¨s API Mastodon** ğŸ˜

Les accÃ¨s API gÃ©nÃ©rÃ©s pour le projet sont :

- **client\_id**: `izb9ti8Xlag6iauwHUAVORXSjL-l554tsFaZvU4RDII`
- **client\_secret**: `kXrzsq8fXTZEyIFbu1mcRCfDWY-8_ZJcLmaWJQRSGLg`
- **access\_token**: `wbhGh7WATwQJ2uHuWnBZ5rQ9SD-idQdk2O93_68Gvis`

## ğŸ“œ **Description des Scripts**

- **create\_app** : Enregistre une nouvelle application cliente ğŸ“ sur Mastodon ğŸ˜ pour obtenir `client_id` et `client_secret`. *(Ne pas exÃ©cuter ğŸš«)*.
- **jeton\_access** : Effectue une demande OAuth ğŸ”‘ pour obtenir un jeton d'accÃ¨s Ã  l'API Mastodon avec les informations d'identification. *(Ne pas exÃ©cuter ğŸš«)*.

### ğŸ“¡ **Partie 1 : Streaming des Toots avec Kafka** ğŸ“¨ (`partie1.ipynb`)

Configure une connexion avec l'API Mastodon ğŸ˜ et Kafka ğŸ“¨, et Ã©coute les toots en temps rÃ©el â±ï¸. Les toots filtrÃ©s sont envoyÃ©s Ã  un sujet Kafka ğŸ“¨.

### âš™ï¸ **Partie 2 : Traitement des Toots avec PySpark** âœ¨ (`partie2.ipynb`)

Configure un pipeline de streaming ğŸš€ avec PySpark âœ¨ pour lire les toots depuis Kafka ğŸ“¨, appliquer des transformations (nettoyage ğŸ§¹, agrÃ©gation par fenÃªtre temporelle â³, etc.) et Ã©crire les rÃ©sultats dans PostgreSQL ğŸ˜.

### ğŸ“Š **Partie 3 : Analyse Historique des Toots** ğŸ“œ (`partie3.ipynb`)

Traite les donnÃ©es de toots historiques ğŸ•’ stockÃ©es dans PostgreSQL ğŸ˜ avec PySpark âœ¨, appliquant des transformations, des agrÃ©gations et optimisations ğŸ”§.

### ğŸš§ **Partie 4 : En Construction** ğŸ—ï¸ (`partie4.ipynb`)

### ğŸ“ˆ **Partie 5 : Visualisations avec Pandas et Matplotlib** ğŸ“Š (`partie5.ipynb`)

GÃ©nÃ¨re des visualisations Ã  partir des donnÃ©es historiques ğŸ•’ en utilisant Pandas ğŸ¼, Matplotlib ğŸ“Š, et Seaborn ğŸŒŠ.

## ğŸ“¦ **Installation des DÃ©pendances**

Les dÃ©pendances nÃ©cessaires sont indiquÃ©es au dÃ©but de chaque script Python ğŸ.

## â–¶ï¸ **Instructions d'ExÃ©cution** ğŸƒâ€â™‚ï¸

1. **Assurez-vous que tous les containers Docker ğŸ‹ sont en marche ğŸ”„.**
2. **ExÃ©cutez les scripts dans l'ordre suivant :**
   - **partie1.ipynb** â†’ **partie2.ipynb** â†’ **partie3.ipynb** â†’ **partie4.ipynb** â†’ **partie5.ipynb**.

## ğŸ“ **Notes**

- Les scripts **create\_app** et **jeton\_access** sont inclus Ã  titre d'exemple, mais ne doivent pas Ãªtre exÃ©cutÃ©s ğŸš«.
- Les accÃ¨s Mastodon ğŸ˜ fournis sont dÃ©jÃ  prÃªts Ã  l'emploi ğŸŸ¢.
- Le fichier tweets_dataset.csv est le fichier d'entrainement qui doit Ãªtre importer pour pouvoir executer la partie 4.
